{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "def retrive_data(filename):\n",
    "    twitter = []\n",
    "    with open(filename,encoding='utf-8') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            twitter.append(item)\n",
    "    totalData = []\n",
    "    for tweets in twitter:  #under one event\n",
    "        TweetList = []\n",
    "        for tweet in tweets:\n",
    "            tweetText = tweet['text']\n",
    "            tweetId = tweet['id']\n",
    "            createTime = time.mktime(datetime.datetime.strptime(tweet[\"created_at\"], \"%a %b %d %H:%M:%S +0000 %Y\").timetuple())\n",
    "            TweetList.append({'id':tweetId, 'text':tweetText, 'time': createTime})\n",
    "        TweetList = sorted(TweetList, key=lambda k: k['time'])        #from early to late\n",
    "        TotalTimeLine = TweetList[-1]['time']-TweetList[0]['time']\n",
    "        \n",
    "        #we only consider source and reply text currently\n",
    "        source_reply =  ' [SEP]'.join([t['text'] for t in TweetList])\n",
    "        totalData.append(source_reply)\n",
    "    return totalData\n",
    "def retrive_label(filename):\n",
    "    id_label = {}\n",
    "    rumor = []\n",
    "    label = []\n",
    "    with open(filename,'r') as f:\n",
    "        temp = json.loads(f.read())\n",
    "        id_label = temp\n",
    "    for i in id_label.values():\n",
    "        rumor.append(i)\n",
    "    for key,val in id_label.items():\n",
    "        if val == 'rumour':\n",
    "            id_label[key] = 0\n",
    "        if val == 'non-rumour':\n",
    "            id_label[key] = 1\n",
    "    for i in id_label.values():\n",
    "        label.append(i)\n",
    "    return id_label,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = retrive_data('dev.data.jsonl')\n",
    "x_train = retrive_data('train.data.jsonl')\n",
    "x_test = retrive_data('test.data.jsonl')\n",
    "y_dev,dev_label = retrive_label('dev.label.json')\n",
    "y_train,train_label = retrive_label('train.label.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_pd = {}\n",
    "dev_pd = {}\n",
    "test_pd = {}\n",
    "train_pd['text'] = [x for x in x_train]\n",
    "train_pd['label'] = [i for i in train_label]\n",
    "dev_pd['text'] = [x for x in x_dev]\n",
    "dev_pd['label'] = [i for i in dev_label]\n",
    "test_pd['text'] = [x for x in x_test]\n",
    "\n",
    "train_df = pd.DataFrame(train_pd)\n",
    "dev_df = pd.DataFrame(dev_pd)\n",
    "test_df = pd.DataFrame(test_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import emoji\n",
    "def clean_dataset(text):\n",
    "    # Remove hashtag while keeping hashtag text\n",
    "    text = re.sub(r'#','', text)\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    text = re.sub(r'\\&\\w*;', '', text)\n",
    "    # Remove tickers\n",
    "    #text = re.sub(r'\\$\\w*', '', text)\n",
    "    # Remove hyperlinks\n",
    "    #text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    text = re.sub(r'\\s\\s+','', text)\n",
    "    text = re.sub(r'[ ]{2, }',' ',text)\n",
    "    # Remove URL, RT, mention(@)\n",
    "    text=  re.sub(r'http(\\S)+', '',text)\n",
    "    text=  re.sub(r'http ...', '',text)\n",
    "    text=  re.sub(r'(RT|rt)[ ]*@[ ]*[\\S]+','',text)\n",
    "    text=  re.sub(r'RT[ ]?@','',text)\n",
    "    text = re.sub(r'@[\\S]+','',text)\n",
    "    # Remove words with 4 or fewer letters\n",
    "    #text = re.sub(r'\\b\\w{1,4}\\b', '', text)\n",
    "    #&, < and >\n",
    "    text = re.sub(r'&amp;?', 'and',text)\n",
    "    text = re.sub(r'&lt;','<',text)\n",
    "    text = re.sub(r'&gt;','>',text)\n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    #text= ''.join(c for c in text if c <= '\\uFFFF') \n",
    "    #text = text.strip()\n",
    "    # Remove misspelling words\n",
    "    #text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    # Remove emoji\n",
    "    text = emoji.demojize(text)\n",
    "    text = text.replace(\":\",\" \")\n",
    "    text = ' '.join(text.split()) \n",
    "    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "    # Remove Mojibake (also extra spaces)\n",
    "    #text = ' '.join(re.sub(\"[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\", \" \", text).split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(clean_dataset)\n",
    "dev_df['text'] =dev_df['text'].apply(clean_dataset)\n",
    "test_df['text'] = test_df['text'].apply(clean_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_label = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sample = 2 * sum(training_label)\n",
    "sweight = [2 if i==0 else 1 for i in training_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6116"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, sour_maxlen, reply_maxlen):\n",
    "\n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = filename\n",
    "\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.sour_maxlen = sour_maxlen\n",
    "        self.reply_maxlen = reply_maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df['text'][index]\n",
    "        label = self.df['label'][index]\n",
    "\n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tweets = sentence.split('[SEP]')\n",
    "        source_tokens = self.tokenizer.tokenize(tweets[0])\n",
    "        source_tokens = ['[CLS]'] + source_tokens\n",
    "        if len(source_tokens) < self.sour_maxlen:\n",
    "            source_tokens = source_tokens + ['[PAD]' for _ in range((self.sour_maxlen-1) - len(source_tokens))] #Padding sentences\n",
    "        source_tokens = source_tokens + ['[SEP]']\n",
    "        \n",
    "        reply_tokens = []\n",
    "        for i in range(1,len(tweets)):\n",
    "            reply = self.tokenizer.tokenize(tweets[i])\n",
    "            reply = reply + ['[SEP]']\n",
    "            if (len(reply) + len(reply_tokens)) < 328:\n",
    "                reply_tokens = reply_tokens + reply\n",
    "        if len(source_tokens) < 328:\n",
    "            reply_tokens = reply_tokens + ['[PAD]' for _ in range(328 - len(reply_tokens))] #Padding sentences\n",
    "\n",
    "        tokens = source_tokens + reply_tokense\n",
    "        \n",
    "        \n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing training and development data.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Creating instances of training and development set\n",
    "#maxlen sets the maximum length a sentence can have\n",
    "#any sentence longer than this length is truncated to the maxlen size\n",
    "train_set = SSTDataset(filename = train_df, sour_maxlen = 60, reply_maxlen = 328)\n",
    "dev_set = SSTDataset(filename = dev_df, sour_maxlen = 60, reply_maxlen = 328)\n",
    "\n",
    "from torch.utils.data.sampler import  WeightedRandomSampler\n",
    "wsampler = WeightedRandomSampler(sweight, num_samples=total_sample, replacement=True)\n",
    "#Creating intsances of training and development dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = 64, num_workers = 2)\n",
    "dev_loader = DataLoader(dev_set, batch_size = 64, num_workers = 2)\n",
    "\n",
    "print(\"Done preprocessing training and development data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class rumour_classifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(rumour_classifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        #Classification layer\n",
    "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        #output dimension is 1 because we're working with a binary classification problem\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the rumor classifier, initialised with pretrained BERT-BASE parameters...\n",
      "Done creating the rumor classifier.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Creating the rumor classifier, initialised with pretrained BERT-BASE parameters...\")\n",
    "net = rumour_classifier()\n",
    "net.cuda() #Enable gpu support for the model\n",
    "print(\"Done creating the rumor classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(net, criterion, opti, train_loader, dev_loader, max_eps):\n",
    "\n",
    "    best_acc = 0\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "        \n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "              \n",
    "            if it % 100 == 0:\n",
    "                \n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "\n",
    "        \n",
    "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def evaluate(net, criterion, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_flag = torch.cuda.is_available()\n",
    "cuda_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 20#epoch 是什么\n",
    "\n",
    "\n",
    "#fine-tune the model\n",
    "train(net, criterion, opti, train_loader, dev_loader, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
